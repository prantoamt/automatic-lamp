{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".....................Welocme to BD Yellow Book Scrapper.....................\n",
      "\n",
      "Connecting to the server....\n",
      "\n",
      "The list below is the list of main catergories. Choose your desired list to download the dataset.\n",
      "\n",
      "\n",
      "0 Agricultural Products & Services\n",
      "1 Art & Culture\n",
      "2 Automotive\n",
      "3 Book, Stationery & Supplies\n",
      "4 Business & Services\n",
      "5 Chemical & Paper\n",
      "6 Childrens Goods\n",
      "7 Community & Organizations\n",
      "8 Computer & Internet\n",
      "9 Consultancy & Construction\n",
      "10 Education & Career\n",
      "11 Electrical & Electronic\n",
      "12 Fashion & Jewelry\n",
      "13 Food & Beverage\n",
      "14 Garments & Accessories\n",
      "15 Health & Medicine\n",
      "16 Household & Handicrafts\n",
      "17 Jute Goods & Tea\n",
      "18 Leather & Leather Goods\n",
      "19 Machinery & Equipment\n",
      "20 Mills & Industries\n",
      "21 News & Media\n",
      "22 Oil & Gas Industries\n",
      "23 Plastic & Rubbers\n",
      "24 Real Estate\n",
      "25 Recreation & Fitness\n",
      "26 Shopping & Greetings\n",
      "27 Telecommunication\n",
      "28 Tour & Travel\n",
      "Choose your desired option: 0\n",
      "\n",
      "Getting  Agricultural Products & Services  dataset..........\n",
      "0 A & A ENTERPRISE\n",
      "1 AARUSH GROUP\n",
      "2 ADVANCE AGROTECH\n",
      "3 AGRO VILLAGE LIMITED\n",
      "4 AMAR COMMUNICATION SYSTEM (ACS)\n",
      "5 APONJON IT LTD.\n",
      "6 ARENA SEED COMPANY LTD.\n",
      "7 BANGLADESH FISHERIES COMMUNITY\n",
      "8 BCS TRADING\n",
      "9 BD LIONS AGRAA.\n",
      "10 BHAI BHAI AUTO RICE MILL LTD.\n",
      "11 COLOUR POINT PRINTING PRESS\n",
      "12 COMPACT AGRO LTD. (CAL)\n",
      "13 CPM DISTRIBUTIONS (PVT.) LTD.\n",
      "14 FORGE AGRO\n",
      "15 FORGETRADING\n",
      "16 FRESH POTATO EXPORTER OF BANGLADESH\n",
      "17 GLOBAL AGRO RESOURCES INCORPORATION\n",
      "18 GRAMMORONG\n",
      "19 GREEN TRADE INTERNATIONAL\n",
      "\n",
      "Total  2 pagers found....\n",
      "http://www.bdyellowbook.com/catalog/Agricultural_Products___Services/index3.html\n",
      "0 RDS BREEDERS\n",
      "1 ROTO ENGINEERING\n",
      "2 SABIR LIMITED\n",
      "3 STRIVE TRADING\n",
      "4 TALHATRAINING\n",
      "5 TUBA ENTERPRISE\n",
      "6 UNITECH BEARING HOUSE\n",
      "http://www.bdyellowbook.com/catalog/Agricultural_Products___Services/index2.html\n",
      "0 GREEN VIEWR AGRO FARM\n",
      "1 HAJ CORPORATION\n",
      "2 HALAL BANGLADESH SERVICES, LTD. (HBS)\n",
      "3 HAMID TECHNO SERVICES\n",
      "4 HUQE FLOUR MILL\n",
      "5 HYBRID RESOURCES\n",
      "6 IDEA AGROBUSINESS MANAGEMENT SERVICES\n",
      "7 IMTIAZ ALAM & COMPANY\n",
      "8 KRISHI BAZAR BD LIMITED\n",
      "9 KRISHI STORE\n",
      "10 LOADS\n",
      "11 LUCKY ENTERPRISE\n",
      "12 M/S NOOR MOHAMMAD\n",
      "13 M/S.ZAKARIA & BROTHERS\n",
      "14 MASTER AGRO\n",
      "15 MAXWELL AGRO INDUSTRIES LIMITED\n",
      "16 NAHAR AGRO GROUP\n",
      "17 PACIFIC IT BD\n",
      "18 PASHA PASHI SEED COMPANY\n",
      "19 QUICK EXPORT IMPORT CENTRE\n",
      "\n",
      "Dataset downloaded successfully.....\n",
      "\n",
      "Preparing your data to save....\n",
      "\n",
      "Dataset saved to your local memory successfully!!....\n"
     ]
    }
   ],
   "source": [
    "import requests as rq\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "r = None\n",
    "downloaded = False\n",
    "\n",
    "def getRequest(url):\n",
    "    url = url\n",
    "    r = rq.get(url)\n",
    "    return r,url\n",
    "    \n",
    "def getIndividualCompanyDetailsLinks(r):\n",
    "    links = []\n",
    "    if(r):\n",
    "        soup = bs(r.content, \"lxml\")\n",
    "        table = soup.find(\"table\",{\"id\":\"item\"})\n",
    "        rows = table.find_all('tr')\n",
    "        for i in range(1,len(rows)):\n",
    "            for a in rows[i].td.find_all('a', href=True):\n",
    "                links.append(a['href'])\n",
    "        return links,soup        \n",
    "    \n",
    "    \n",
    "def addToDataFrame(linkss,url):\n",
    "    df = pd.DataFrame(columns=(\"Company name\", \"Products and services\", \"Address\", \"City\", \"Phone\", \"Fax\", \"Web\", \"Desc\") )\n",
    "    for i in range(0,len(linkss)):\n",
    "        url_individual = url+linkss[i]\n",
    "        r1 = rq.get(url_individual)\n",
    "        s = bs(r1.content, 'lxml')\n",
    "        table = s.find(\"table\", {\"id\":\"item\"})\n",
    "        rows = table.find_all(\"tr\")\n",
    "        td = rows[1].find_all('td')\n",
    "        c_name = td[0].text.strip()\n",
    "        if(not c_name):\n",
    "            c_name = \"None\"\n",
    "        td = rows[3].find_all('td')\n",
    "        product_service = td[1].text.strip()\n",
    "        if(not product_service):\n",
    "            product_service = \"None\"\n",
    "        td = rows[4].find_all('td')\n",
    "        address = td[1].text.strip()\n",
    "        if(not address):\n",
    "            address = \"None\"\n",
    "        td = rows[5].find_all('td')\n",
    "        city = td[1].text.strip()\n",
    "        if(not city):\n",
    "            city = \"None\"\n",
    "        td = rows[6].find_all('td')\n",
    "        phone = td[1].text.strip()\n",
    "        if(not phone):\n",
    "            phone = \"None\"\n",
    "        td = rows[7].find_all('td')\n",
    "        fax = td[1].text.strip()\n",
    "        if(not fax):\n",
    "            fax = \"None\"\n",
    "        td = rows[10].find_all('td')\n",
    "        web = td[1].text.strip()\n",
    "        if(not web):\n",
    "            web = \"None\"\n",
    "        td = rows[9].find_all('td')\n",
    "        desc = td[1].text.strip()\n",
    "        if(not desc):\n",
    "            desc = \"None\"\n",
    "        print(i,c_name)\n",
    "        df = df.append({'Company name': c_name, 'Products and services': product_service, 'Address': address,'City': city, 'Phone': phone, 'Fax': fax, \n",
    "              'Web': web,'Desc': desc}, ignore_index=True)\n",
    "    return df\n",
    "\n",
    "def getPagers(soup):\n",
    "    pagers = []\n",
    "    for a in soup.find_all('a',{\"class\":\"pager\"},href=True):\n",
    "        pagers.append(a['href'])\n",
    "    pagers.remove(pagers[-1])    \n",
    "    return pagers    \n",
    "    \n",
    "# 1,3,4,5,6,7,10,9        \n",
    "\n",
    "def getPagerData(pager):\n",
    "    req1, url1 = getRequest(pager)\n",
    "    print(url1)\n",
    "    linkss1, soup = getIndividualCompanyDetailsLinks(req1)\n",
    "    df2 = addToDataFrame(linkss1,url1)\n",
    "    return df2\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "##..........................##\n",
    "\n",
    "yellow_book = \"http://www.bdyellowbook.com\" \n",
    "print(\".....................Welocme to BD Yellow Book Scrapper.....................\")\n",
    "print(\"\\nConnecting to the server....\")\n",
    "request, url = getRequest(\"http://www.bdyellowbook.com/\")\n",
    "if(request.status_code==200):\n",
    "    b_soup = bs(request.content, 'lxml')\n",
    "    catalog_table = b_soup.find(\"table\", {\"id\":\"catalog\"})\n",
    "    main_categories_link =  []\n",
    "    main_categories_name = []\n",
    "    print(\"\\nThe list below is the list of main catergories. Choose your desired list to download the dataset.\\n\\n\")\n",
    "    for a in catalog_table.find_all(\"a\",href=True):\n",
    "        link = yellow_book+a['href']\n",
    "        print(len(main_categories_link),a.string)\n",
    "        main_categories_name.append(a.string)\n",
    "        main_categories_link.append(link)\n",
    "\n",
    "    select = int(input(\"Choose your desired option: \"))\n",
    "    print(\"\\nGetting \", main_categories_name[select], \" dataset..........\")\n",
    "    req, url = getRequest(main_categories_link[select])\n",
    "    linkss, soup = getIndividualCompanyDetailsLinks(req)\n",
    "    df = addToDataFrame(linkss,url)\n",
    "    \n",
    "    pagers = getPagers(soup)\n",
    "    print(\"\\nTotal \",len(pagers), \"pagers found....\")\n",
    "    for i in range(0,len(pagers)):\n",
    "        new_df = getPagerData(pagers[i])\n",
    "        df = df.append(new_df,ignore_index=True)\n",
    "    print(\"\\nDataset downloaded successfully.....\")\n",
    "    downloaded = True\n",
    "else:\n",
    "    print(\"\\nFailed to connect with the server. Please check your internet connection and try again.\")\n",
    "\n",
    "    \n",
    "if(downloaded==True):\n",
    "    print(\"\\nPreparing your data to save....\")\n",
    "    df1 = df[(df['Web']==\"None\") & (df[\"City\"]==\"Dhaka\")]\n",
    "    with pd.ExcelWriter(main_categories_name[select]+'.xlsx') as writer:  \n",
    "        df.to_excel(writer, sheet_name='Total Data')\n",
    "        df1.to_excel(writer, sheet_name='Without Web and inside dhaka')\n",
    "    print(\"\\nDataset saved to your local memory successfully!!....\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Company name             29\n",
       "Products and services    29\n",
       "Address                  29\n",
       "City                     29\n",
       "Phone                    29\n",
       "Fax                      29\n",
       "Web                      29\n",
       "Desc                     29\n",
       "dtype: int64"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
